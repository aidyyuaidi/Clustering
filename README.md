# Clustering
Unsupervised learning part for the credit project.

##SQL Server Connect

The data is acquired from SQL Server. We will need to set up the ODBC connect mannualy, and connect through R.
```R
library(RODBC)
# Connect to SQL Server
con <- odbcConnect("..", uid = "..", pwd = "..")
# Get the data using SQL Language
dat <- sqlQuery(con,"select * from ..")
```
The above code should return a data frame in R.

## Assessing Clustering Tendency
Before applying any clustering method on the data, it’s important to evaluate whether the data contains meaningful clusters (i.e.: non-random structures) or not. This process is defined as *the assessing of clustering tendency* or *the feasibility of the clustering analysis*.

A big issue, in cluster analysis, is that clustering methods will return clusters even if the data does *NOT* contain any clusters. In other words, if you blindly apply a clustering method on a data set, it will divide the data into clusters because that is what it supposed to do.

### Required R packages
* *factoextra* for data visualization
* *clustertend* for statistical assessment clustering tendency
```R
library(factoextra)
library(clustertend)
```

### Data Preparation
For the purpose of illustration, we will use two data sets:
* the built-in R data set *iris*.
* a *random data set* generated from the iris data set.
```R
# Prepairing data iris
data(iris)
traindat <- iris
traindat$Species <- NULL
# Prepairing random data
random <- apply(traindat, MARGIN = 2, function(x) {runif(length(x), min(x), (max(x)))})
random <- as.data.frame(random)
# Scale the two data sets
traindat <- scale(traindat)
random <- scale(random)
```
Note that both data sets contain 150 rows and 4 columns.

### Visual Inspection of the Data
We start by visualizing the data to assess whether they contains any meaningful clusters.

As the data contain more than two variables, we need to reduce the dimensionality in order to plot a scatter plot. This can be done using principal component analysis (PCA) algorithm. After performing PCA, we visualize the output.

```R
# For iris data
fviz_pca_ind(prcomp(traindat), title = "PCA - Iris data", habillage = iris$Species,
             palette = "jco", geom = "point", ggtheme = theme_classic(), legend = "bottom")
# For random data
fviz_pca_ind(prcomp(random), title = "PCA - Random data", geom = "point", 
             ggtheme = theme_classic())
```
The visualizations can provide us a general idea of whether the data can be clustered. If the data does not seem to form groups, clustering analysis might not be as much meaningful.

### Methods for Assessing Clustering Tendency
The *Hopkins Statistic* is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution. In other words, it tests the spatial randomness of the data.

The null and the alternative hypotheses are defined as follow:

* **Null Hypothesis**: the data set is uniformly distributed (i.e., no meaningful clusters).
* **Alternative Hypothesis**: the data set is not uniformly distributed (i.e., contains meaningful clusters).

We can use 0.5 as the threshold to reject the alternative hypothesis. That is, if the value of Hopkins statistic > 0.5, then it is unlikely that D has statistically significant clusters. Put in other words, If the value of Hopkins statistic is close to zero, then we can reject the null hypothesis and conclude that the dataset D is significantly a clusterable data.

```R
set.seed(123)
hopkins(traindat, n = nrow(traindat) - 1)
hopkins(random, n = nrow(random) - 1)
```

## Choosing the Best Clustering Algorithms

After confirming that the data can be clustered, the next step is to choose the clustering algorithm that best suits the data situation. **Choosing the best clustering method** for a given data can be a hard task for the analyst. In this section, we will use the R package **clValid**, which can be used to compare simultaneously multiple clustering algorithms in a single function call for identifying the best clustering approach and the optimal number of clusters.

```R
library(clValid)
clm <- c("hierarchical","kmeans","diana","fanny","som","model","sota","pam","clara","agnes")
vali <- c("internal", "stability", "biological")
compare <- clValid(traindat, nClust = 2:10, clMethods =  clm, validation = vali, 
                   maxitems = 600, metric = "euclidean", method = "average")
# Parameters:
## nClust: A numeric vector giving the numbers of clusters to be evaluated.
## maxitems: The maximum number of items (rows in matrix) which can be clustered.
## metric: Determine the distance matrix. Choose from "euclidean", "correlation", and "manhattan".
## method: For hierarchical clustering, the agglomeration method used. Available choices are "ward", "single", "complete", and "average".
optimalScores(compare)
```

## Clustering

### 1. K-Means

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
kmclu <- kmeans(traindat, centers = 3, iter.max = 300, nstart = 20)
library("factoextra")
fviz_cluster(kmclu, data = traindat, ellipse.type = "t", geom = "point", pointsize = 1,
             ggtheme = theme_classic())
table(iris$Species, kmclu$cluster)
```

### 2. PAM

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
library(cluster)
pamclu <- pam(traindat, k = 3)
fviz_cluster(pamclu, data = traindat, ellipse.type = "t", geom = "point", pointsize = 1,
             ggtheme = theme_classic())
table(iris$Species, pamclu$clustering)
```

### 3. CLARA

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
library(cluster)
claraclu <- clara(traindat, k = 3)
fviz_cluster(claraclu, data = traindat, ellipse.type = "t", geom = "point", pointsize = 1,
             ggtheme = theme_classic())
table(iris$Species, claraclu$clustering)
```

### 4. Affinity Propagation (AP)

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
library(apcluster)
apclu <- apcluster(negDistMat(r=2), traindat)
# PCA for data visulization
irispca <- princomp(traindat, cor = T)
ptdat <- predict(irispca)[,c(1,2)]
plot(apclu, ptdat)
table(iris$Species, apclu@idx)
```

### 5. Mean Shift

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
# Transpose of the data so that the features are on the horizontal lines
traindat <- t(traindat)
library(MeanShift)
# Use multiple cores
options(mc.cores = 2)
msclu <- msClustering(traindat, h = 0.8, multi.core=TRUE)
traindat <- iris
traindat$Species <- NULL
irispca <- princomp(traindat, cor = T)
ptdat <- predict(irispca)[,c(1,2)]
plot(ptdat, col = msclu$labels, cex=0.8, pch=16)
table(iris$Species, msclu$labels)
```

### 6. Spectral Clustering

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
traindat <- as.matrix(traindat)
library(kernlab)
spclu <- specc(traindat, centers = 3)
irispca <- princomp(traindat, cor = T)
ptdat <- predict(irispca)[,c(1,2)]
plot(ptdat, col = spclu)
table(iris$Species, spclu@.Data)
```

### 7. Hierarchical Clustering

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
# Calculate the distances
traindat <- dist(traindat)
# Clustering
hclu <- hclust(traindat, method = "ward.D2")
library("factoextra")
fviz_dend(hclu, k = 3, cex = 0.5, k_colors = c(1, 2, 3), color_labels_by_k = TRUE, 
          rect = TRUE)
hresult <- cutree(hclu, k = 3)
fviz_cluster(list(data = traindat, cluster = hresult), palette = c(1, 2, 3), 
             ellipse.type = "convex", show.clust.cent = FALSE, ggtheme = theme_minimal())
table(iris$Species, hresult)
```

### 8. Agglomerative Clustering

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
library(cluster)
agclu <- agnes(traindat, method = "average")
agresult <- cutree(agclu, k = 3)
fviz_cluster(list(data = traindat, cluster = agresult), palette = c(1, 2, 3), 
             ellipse.type = "convex", show.clust.cent = FALSE, ggtheme = theme_minimal())
table(iris$Species, agresult)
```

### 9. DBSCAN

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
library(fpc)
dbclu <- dbscan(traindat, eps = 0.6, MinPts = 4)
irispca <- princomp(traindat, cor = T)
ptdat <- predict(irispca)[,c(1,2)]
plot(dbclu, ptdat)
dbresult <- dbclu$cluster
table(iris$Species, dbresult)
```

### 10. Gaussian Mixtures

```R
data(iris)
traindat <- iris
traindat$Species <- NULL
library(mclust)
mc <- Mclust(traindat, G = 3)
irispca <- princomp(traindat, cor = T)
ptdat <- predict(irispca)[,c(1,2)]
mclust2Dplot(ptdat, parameters = mc$parameters, z = mc$z, what = "classification", main = T)
mcresult <- mc$classification
table(iris$Species, mcresult)
```



---

References：

<http://www.sthda.com/english/articles/29-cluster-validation-essentials/95-assessing-clustering-tendency-essentials/>
